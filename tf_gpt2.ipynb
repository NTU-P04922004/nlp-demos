{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "Dsnr1ewu1Y28",
        "tG5l2X9y1eyu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Demo: Training GPT-2 with TensorFlow and Keras\n",
        "\n",
        "This notebook demonstrate training a GPT-2 model from scratch with TensorFlow and Keras. We will will train the model on a text dataset and make it generate similar texts.\n",
        "\n",
        "\n",
        "Note that training GPT-2 models can be computationally expensive, and using a GPU is highly recommended.\n",
        "\n",
        "**References**:\n",
        "- [nanoGPT](https://github.com/karpathy/nanoGPT)\n",
        "- [Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)"
      ],
      "metadata": {
        "id": "RHkSPSYz7pqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Environment"
      ],
      "metadata": {
        "id": "Dsnr1ewu1Y28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required packages"
      ],
      "metadata": {
        "id": "qT6r66ae8ZRa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT5PRAR74C_y"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.16.1 keras==3.1.1 keras-nlp==0.8.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download dataset"
      ],
      "metadata": {
        "id": "ExtwFwE38cGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "tMC2rZYeqcXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and Define Global Constants"
      ],
      "metadata": {
        "id": "tG5l2X9y1eyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "\n",
        "import keras\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from keras import ops\n",
        "from keras.layers import (Dense, Dropout, Embedding, Input, Layer, LayerNormalization,\n",
        "                          MultiHeadAttention, TextVectorization)\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'"
      ],
      "metadata": {
        "id": "FrkFKBli58nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IS_TRAIN = True\n",
        "MIXED_PRECISION = False     # Use mixed precision for training.\n",
        "DETERMINISM = True          # Run in deterministic mode.\n",
        "FREEZE_BACKBONE = False\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "SEED = 42\n",
        "VOCABULARY_SIZE = 20000\n",
        "BATCH_SIZE = 128\n",
        "MAX_SEQ_LENGTH = 80\n",
        "EMBEDDING_DIM = 256\n",
        "NUM_LAYERS = 1\n",
        "NUM_ATTENTION_HEADS = 2\n",
        "TRAIN_EPOCHS = 2\n",
        "LEARNING_RATE = 1e-3"
      ],
      "metadata": {
        "id": "8fK3u8Si1ipv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MIXED_PRECISION:\n",
        "    keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "if DETERMINISM:\n",
        "    keras.utils.set_random_seed(SEED)"
      ],
      "metadata": {
        "id": "nTSgSHHB1l3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data\n",
        "\n",
        "In this demo, we will use the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/). The dataset is originally for binary sentiment classification that has a set of 25,000 movie reviews for training, and 25,000 for testing. Besides, the dataset contains each review in a separate text file.\n"
      ],
      "metadata": {
        "id": "gKMJSLL8iDJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercased = tf.strings.lower(input_string)\n",
        "    stripped_html = tf.strings.regex_replace(lowercased, '<br />', ' ')\n",
        "    return tf.strings.regex_replace(stripped_html, f'([{string.punctuation}])', r' \\1')\n",
        "\n",
        "\n",
        "def prepare_lm_inputs_labels(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    inputs = tokenized_sentences[:, :-1]\n",
        "    labels = tokenized_sentences[:, 1:]\n",
        "    return inputs, labels\n",
        "\n",
        "\n",
        "def create_dataset(file_paths, is_training=False):\n",
        "    dataset = tf.data.TextLineDataset(sorted(file_paths))\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    if is_training:\n",
        "        vectorize_layer.adapt(dataset)\n",
        "        dataset = dataset.shuffle(buffer_size=256, seed=SEED)\n",
        "\n",
        "    dataset = dataset.map(prepare_lm_inputs_labels,\n",
        "                          num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.prefetch(AUTOTUNE)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "mue_VCFAil-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_paths = []\n",
        "val_file_paths = []\n",
        "data_directories = [\n",
        "    'aclImdb/train/pos',\n",
        "    'aclImdb/train/neg',\n",
        "    'aclImdb/test/pos',\n",
        "    'aclImdb/test/neg',\n",
        "]\n",
        "\n",
        "for directory in data_directories:\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        if 'train' in directory:\n",
        "            train_file_paths.append(file_path)\n",
        "        else:\n",
        "            val_file_paths.append(file_path)\n",
        "\n",
        "print(f'{len(train_file_paths)} train files, {len(val_file_paths)} validation files')\n",
        "\n",
        "# Create the text vectorization layer.\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=VOCABULARY_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQ_LENGTH + 1,\n",
        "    pad_to_max_tokens=True\n",
        ")\n",
        "\n",
        "# Create training and validation datasets.\n",
        "train_ds = create_dataset(train_file_paths, is_training=True)\n",
        "val_ds = create_dataset(val_file_paths)"
      ],
      "metadata": {
        "id": "yA78OCOhiOwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Show samples in the train dataset:')\n",
        "for i, (data_batch, label_batch) in enumerate(train_ds.take(2)):\n",
        "    print(f'Sample #{i + 1}')\n",
        "    print(f'    text: {data_batch.numpy()}')\n",
        "    print(f'    label: {label_batch.numpy()}')"
      ],
      "metadata": {
        "id": "O-LS2r6zj_AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model"
      ],
      "metadata": {
        "id": "tCNyXSg_hWX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_2_kernel_initializer(stddev=0.02):\n",
        "    return keras.initializers.RandomNormal(stddev=stddev)\n",
        "\n",
        "\n",
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    i = ops.arange(n_dest)[:, None]\n",
        "    j = ops.arange(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = ops.cast(m, dtype)\n",
        "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = ops.concatenate(\n",
        "        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n",
        "    )\n",
        "    return ops.tile(mask, mult)\n",
        "\n",
        "\n",
        "class MLP(Layer):\n",
        "    def __init__(self, hidden_dim, intermediate_dim, dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense_1 = Dense(\n",
        "            intermediate_dim,\n",
        "            use_bias=False,\n",
        "            kernel_initializer=gpt_2_kernel_initializer()\n",
        "        )\n",
        "        self.dense_2 = Dense(\n",
        "            hidden_dim,\n",
        "            use_bias=False,\n",
        "            kernel_initializer=gpt_2_kernel_initializer(\n",
        "                0.02 / math.sqrt(2 * 1))\n",
        "        )\n",
        "        self.dropout = Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense_1(inputs)\n",
        "        x = keras.activations.gelu(x, approximate=True)\n",
        "        x = self.dense_2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim,\n",
        "        intermediate_dim,\n",
        "        num_heads,\n",
        "        dropout=0.1,\n",
        "        layer_norm_epsilon=1e-05,\n",
        "        **kwargs\n",
        "    ):\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "        self.att = MultiHeadAttention(\n",
        "            num_heads,\n",
        "            hidden_dim,\n",
        "            dropout=dropout,\n",
        "            use_bias=False,\n",
        "            kernel_initializer=gpt_2_kernel_initializer()\n",
        "        )\n",
        "        self.mlp = MLP(\n",
        "            hidden_dim=hidden_dim,\n",
        "            intermediate_dim=intermediate_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.layernorm_1 = LayerNormalization(epsilon=layer_norm_epsilon)\n",
        "        self.layernorm_2 = LayerNormalization(epsilon=layer_norm_epsilon)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size, seq_len = ops.shape(inputs)[:2]\n",
        "        causal_mask = causal_attention_mask(\n",
        "            batch_size, seq_len, seq_len, 'bool')\n",
        "        pre_norm = self.layernorm_1(inputs)\n",
        "        attention_output = self.att(\n",
        "            pre_norm, pre_norm, attention_mask=causal_mask)\n",
        "        mid_out = inputs + attention_output\n",
        "        post_norm = self.layernorm_2(mid_out)\n",
        "        return mid_out + self.mlp(post_norm)\n",
        "\n",
        "\n",
        "class GPT2Model(keras.Model):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_emb = Embedding(\n",
        "            input_dim=config.vocab_size,\n",
        "            output_dim=config.n_embd,\n",
        "            embeddings_initializer=gpt_2_kernel_initializer(),\n",
        "            name='token_embedding'\n",
        "        )\n",
        "        self.pos_emb = Embedding(\n",
        "            input_dim=config.n_positions,\n",
        "            output_dim=config.n_embd,\n",
        "            embeddings_initializer=gpt_2_kernel_initializer(),\n",
        "            name='position_embedding'\n",
        "        )\n",
        "        self.dropout = Dropout(config.embd_pdrop)\n",
        "        self.transformer_layers = [\n",
        "            TransformerBlock(\n",
        "                hidden_dim=config.n_embd,\n",
        "                intermediate_dim=config.n_embd * 4,\n",
        "                num_heads=config.n_head,\n",
        "                dropout=config.resid_pdrop,\n",
        "                layer_norm_epsilon=config.layer_norm_epsilon,\n",
        "                name=f'transformer_layer_{i}'\n",
        "            ) for i in range(config.n_layer)\n",
        "        ]\n",
        "        self.layer_norm = LayerNormalization(\n",
        "            axis=-1,\n",
        "            epsilon=config.layer_norm_epsilon,\n",
        "            name='layer_norm'\n",
        "        )\n",
        "        self.lm_head = Dense(\n",
        "            config.vocab_size,\n",
        "            use_bias=False,\n",
        "            kernel_initializer=gpt_2_kernel_initializer(),\n",
        "            name='lm_head'\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        maxlen = ops.shape(inputs)[-1]\n",
        "        positions = ops.arange(0, maxlen, 1)\n",
        "        pos_emb = self.pos_emb(positions)\n",
        "        token_emb = self.token_emb(inputs)\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.lm_head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT2Config:\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=50257,\n",
        "        n_positions=1024,\n",
        "        n_embd=768,\n",
        "        n_layer=12,\n",
        "        n_head=12,\n",
        "        resid_pdrop=0.1,\n",
        "        embd_pdrop=0.1,\n",
        "        attn_pdrop=0.1,\n",
        "        layer_norm_epsilon=1e-5\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_positions = n_positions\n",
        "        self.n_embd = n_embd\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.resid_pdrop = resid_pdrop\n",
        "        self.embd_pdrop = embd_pdrop\n",
        "        self.attn_pdrop = attn_pdrop\n",
        "        self.layer_norm_epsilon = layer_norm_epsilon\n"
      ],
      "metadata": {
        "id": "uV8hUZiXitn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(config):\n",
        "    inputs = Input(shape=(config.n_positions,), dtype='int32')\n",
        "    model = GPT2Model(config)\n",
        "    return model"
      ],
      "metadata": {
        "id": "dEyyX3wklZMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config(\n",
        "    vocab_size=VOCABULARY_SIZE,\n",
        "    n_positions=MAX_SEQ_LENGTH,\n",
        "    n_layer=NUM_LAYERS,\n",
        "    n_head=NUM_ATTENTION_HEADS,\n",
        "    n_embd=EMBEDDING_DIM,\n",
        "    resid_pdrop=0.0,\n",
        "    embd_pdrop=0.0,\n",
        "    attn_pdrop=0.0\n",
        ")\n",
        "\n",
        "model = create_model(config)"
      ],
      "metadata": {
        "id": "ZmP-uMBA7Q0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "XzOn4Rzo7KkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = None\n",
        "if IS_TRAIN:\n",
        "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.AdamW(LEARNING_RATE),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[perplexity]\n",
        "    )\n",
        "\n",
        "    history = model.fit(train_ds,\n",
        "                        validation_data=val_ds,\n",
        "                        epochs=TRAIN_EPOCHS,\n",
        "                        verbose=1)\n",
        "\n",
        "    model.save_weights(f'/content/final.weights.h5')"
      ],
      "metadata": {
        "id": "Gmzos5oBno5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Training Process"
      ],
      "metadata": {
        "id": "Y0KOmZ2Q_nfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if history is not None:\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Loss History')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history.history['perplexity'])\n",
        "    plt.plot(history.history['val_perplexity'])\n",
        "    plt.title('Perplexity History')\n",
        "    plt.ylabel('perplexity')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WG6LJZG09611"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Text"
      ],
      "metadata": {
        "id": "beNFbUa4H5z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_from_top_k(logits, top_k=3):\n",
        "    logits, indices = ops.top_k(logits, k=top_k, sorted=True)\n",
        "    indices = np.asarray(indices).astype('int32')\n",
        "    preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
        "    preds = np.asarray(preds).astype('float32')\n",
        "    return np.random.choice(indices, p=preds)\n",
        "\n",
        "\n",
        "def generate_text(model, index_to_word, prompt_tokens, num_tokens, max_num_tokens):\n",
        "    num_tokens_generated = 0\n",
        "    tokens_generated = []\n",
        "    current_tokens = prompt_tokens.copy()\n",
        "    while num_tokens_generated <= num_tokens:\n",
        "        pad_len = max_num_tokens - len(current_tokens)\n",
        "        sample_index = len(current_tokens) - 1\n",
        "        if pad_len < 0:\n",
        "            x = current_tokens[:max_num_tokens]\n",
        "            sample_index = max_num_tokens - 1\n",
        "        elif pad_len > 0:\n",
        "            x = current_tokens + [0] * pad_len\n",
        "        else:\n",
        "            x = current_tokens\n",
        "        x = np.array([x])\n",
        "        model_output = model.predict(x, verbose=0)\n",
        "        next_token = sample_from_top_k(model_output[0][sample_index], top_k=3)\n",
        "        tokens_generated.append(next_token)\n",
        "        current_tokens.append(next_token)\n",
        "        num_tokens_generated += 1\n",
        "\n",
        "    result = ' '.join(\n",
        "        [index_to_word[token_id]\n",
        "            for token_id in current_tokens + tokens_generated]\n",
        "    )\n",
        "    return result"
      ],
      "metadata": {
        "id": "-7PdPCy_H-GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/final.weights.h5')\n",
        "\n",
        "vocabulary = vectorize_layer.get_vocabulary()\n",
        "\n",
        "word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
        "\n",
        "start_prompt = 'this movie'\n",
        "prompt_tokens = [word_to_index.get(w, 1) for w in start_prompt.split()]\n",
        "num_tokens = 40\n",
        "\n",
        "keras.utils.set_random_seed(SEED)\n",
        "result = generate_text(model, vocabulary, prompt_tokens, num_tokens, MAX_SEQ_LENGTH)\n",
        "\n",
        "print(f'generated text:\\n{result}\\n')"
      ],
      "metadata": {
        "id": "Bm9VyMzoIJCu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}